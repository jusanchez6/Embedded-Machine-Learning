{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SFBFiQlYlva"
      },
      "source": [
        "# Embedded ML - Lab 2.3: TensorFlow Lite Micro"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTNEs3wxwHHO"
      },
      "source": [
        "Tensor Flow Lite Micro (TFLM) is a library that aims to run ML models efficiently on embedded systems. It's a C++ library that provides a version of the TensorFlow Lite interpreter that supports less types of operations and uses less memory. The library also provides helper functions for data pre- and post-processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# To run this notebook, locally as a jupyter notebook, you need to install thhe proper packages.\n",
        "# follow the instructions below to set up your environment.\n",
        "\n",
        "\n",
        "# 1. Crea un entorno virtual usando conda o venv\n",
        "#    Por ejemplo, usando conda:\n",
        "#        conda create -n [myenv] python=3.8\n",
        "#        conda activate [myenv]\n",
        "#    O usando venv:\n",
        "#        python3 -m venv [myenv]\n",
        "#        source [myenv]/bin/activate\n",
        "#\n",
        "# 2. Activa el entorno virtual\n",
        "# 3. Instala los paquetes requeridos usando pip\n",
        "# 4. Ejecuta el notebook\n",
        "\n",
        "\n",
        "# Instala los paquetes requeridos:\n",
        "%pip install numpy -q\n",
        "%pip install pandas -q\n",
        "%pip install matplotlib -q\n",
        "%pip install tensorflow -q\n",
        "%pip install scikit-learn -q\n",
        "%pip install tensorflow-hub -q\n",
        "%pip install tensorflow-datasets -q\n",
        "%pip install tensorflow-estimator -q\n",
        "%pip install roboflow -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading Dataset Version Zip in Rock-Paper-Scissors-1 to folder:: 100%|██████████| 30897/30897 [00:46<00:00, 662.62it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to Rock-Paper-Scissors-1 in folder:: 100%|██████████| 2939/2939 [00:00<00:00, 8201.19it/s]\n"
          ]
        }
      ],
      "source": [
        "from roboflow import Roboflow\n",
        "\n",
        "\n",
        "rf = Roboflow(api_key=\"w5hJLcBvvd3Y3rBjxkkU\")\n",
        "project = rf.workspace(\"eml-pb5ag\").project(\"rock-paper-scissors-vwkqp\")\n",
        "version = project.version(1)\n",
        "dataset = version.download(\"folder\")\n",
        "                "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQK0RRRuY3rJ"
      },
      "source": [
        "### Learning outcomes\n",
        "\n",
        "\n",
        "* Explain the basic concepts associated with TFLM\n",
        "* Use the API to implement the TFLM workflow for an embedded application\n",
        "* Execute TFLM code on a microcontroller-based embedded system"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8wat6Kxul5R"
      },
      "source": [
        "### TensorFlow Lite Micro workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7yMmdHVGlBQ"
      },
      "source": [
        "TFLM's high-level workflow is rather simple:\n",
        "* Generate a small TensorFlow model that can fit your target device and contains supported operations.\n",
        "* Convert to a TensorFlow Lite model using the TensorFlow Lite converter, applying quantization if required.\n",
        "* Convert to a C byte array using standard tools and stored it in the read-only program memory on device.\n",
        "* Run inference on device using the TFLM C++ library and process the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7k1FsSjhAYt"
      },
      "source": [
        "### Hello World and Hello Human"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hd3ZdaAnhFYd"
      },
      "source": [
        "After installing the Arduino IDE and the board files, you should install the Harvard_TinyMLx library that contains the TensorFlow Lite Micro and other resources and examples to build ML apps with Arduino and TFLM. Later on, depending on the application you want to build and the specific hardware to be used, you should install the propper peripheral drivers for communication, sensing and actuating."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qw2a-MZTh3Za"
      },
      "source": [
        "\n",
        "\n",
        "*   Install Arduino IDE 2 from: https://downloads.arduino.cc/arduino-ide/arduino-ide_2.3.2_Linux_64bit.AppImage\n",
        "*   From the boards manager install: Arduino Mbed OS Nano boards\n",
        "*   Allow the linux user to access serial port: `sudo usermod -a -G dialout \\<username\\>` (reboot afterwards)\n",
        "*   From the library manager install: Harvard_TinyMLx\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpML00lXjmIe"
      },
      "source": [
        "Now open the **Hello World** example from the Harvard_TinyMLx library File->Examples->Harvard_TinyMLx in Arduino IDE (also available in [this repo](https://github.com/tinyMLx/arduino-library/tree/main/examples/hello_world)), compile it and run it on the microcontroller board. It is an ML model to predict a sine wave that is used to dim on and off an LED. The Arduino IDE serial monitor should also show interger numbers up and down trying to model a sine wave. This is a test app to make sure that the basic HW and SW elements, including TFLM, are working."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYHUC3Jkpb5o"
      },
      "source": [
        "Inspect the code to make sure you identify and understand the main parts of the workflow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drf1xEruN1E2"
      },
      "source": [
        "Running on-device inference using the TFLM C++ library usually involves:\n",
        "\n",
        "* Include the library headers\n",
        "* Include the model header\n",
        "* Load a model\n",
        "* Instantiate operations resolver\n",
        "* Allocate memory\n",
        "* Instantiate interpreter\n",
        "* Read and pre-process input data\n",
        "* Provide inputs to the allocated tensors\n",
        "* Run inference\n",
        "* Get results from the output tensors\n",
        "* Take action based on outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hl9oeI3e7Aq7"
      },
      "source": [
        "After you have succesfully run the Hello World example, move on to running the **Person Detection** example from the same library. Explore the code in detail to understand how to handle the **camera**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNXSXWHrsJ9x"
      },
      "source": [
        "### TinyML application development"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7a8wQ45sT3c"
      },
      "source": [
        "ML applications that run on embeded systems with very limited resources are often called TinyML. In this lab the goal is to develop a simple TinyML application that uses computer vision up to its deployment on the target embedded device: **Arduino Nano 33 BLE.**\n",
        "\n",
        "Follow these steps in order to develop your TinyML application:\n",
        "\n",
        "1. Select two visual objects that are radically different and  assemble a dataset that contains at least hundreds or thousands of examples. You can create the images yourslef or extract them from a public database and apply data augmentation.\n",
        "\n",
        "2. Design and train a model to classify between the two chosen objects. You can build a dense or CNN model from scratch, or use transfer learning, but you should always keep in mind the very limited memory resources of the target device as well as the image properties of the embedded camera.\n",
        "\n",
        "3.   Export the trained model to a file and convert it to a C header by running the following linux command: `xxd -i converted_model.tflite > converted_model_data.h`\n",
        "\n",
        "4.   Develop an Arduino code based on the Hello World and Person Detection examples, to detect whether any of the two objects are present on the camera view. Indicate the result through the RGB LED.\n",
        "\n",
        "Include in your notebook submission both the code you developed to build the model as well as the C++ codes for the MCU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the necessary libraries\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "import os\n",
        "import logging\n",
        "import sys\n",
        "import contextlib\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "tf.get_logger().setLevel(logging.ERROR)\n",
        "\n",
        "@contextlib.contextmanager\n",
        "def suppress_stdout():\n",
        "    with open(os.devnull, 'w') as devnull:\n",
        "        old_stdout = sys.stdout\n",
        "        sys.stdout = devnull\n",
        "        try:\n",
        "            yield\n",
        "        finally:\n",
        "            sys.stdout = old_stdout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">638</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">478</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">319</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">239</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">317</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">237</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">584</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">158</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">118</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">149152</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │       <span style=\"color: #00af00; text-decoration-color: #00af00\">447,459</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m638\u001b[0m, \u001b[38;5;34m478\u001b[0m, \u001b[38;5;34m8\u001b[0m)    │            \u001b[38;5;34m80\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m319\u001b[0m, \u001b[38;5;34m239\u001b[0m, \u001b[38;5;34m8\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m317\u001b[0m, \u001b[38;5;34m237\u001b[0m, \u001b[38;5;34m8\u001b[0m)    │           \u001b[38;5;34m584\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_5 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m158\u001b[0m, \u001b[38;5;34m118\u001b[0m, \u001b[38;5;34m8\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m149152\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │       \u001b[38;5;34m447,459\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">448,123</span> (1.71 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m448,123\u001b[0m (1.71 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">448,123</span> (1.71 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m448,123\u001b[0m (1.71 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Tensor flow model \n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Input(shape=(640, 480, 1)),\n",
        "    tf.keras.layers.Conv2D(8, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Conv2D(8, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(3, activation='softmax'),  \n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
