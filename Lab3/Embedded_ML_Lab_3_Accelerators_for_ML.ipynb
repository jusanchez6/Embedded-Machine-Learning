{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SFBFiQlYlva"
      },
      "source": [
        "# Embedded ML - Lab 3: Accelerators for ML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTNEs3wxwHHO"
      },
      "source": [
        "Tensor Flow Lite is not only a vehicle to export models to be used in Tensor Flow Lite Micro. It's also an interpreter to run inference on devices that have less resources than servers or standard computers, such as mobile phones, drones, network cameras, etc. In this lab we will use TF Lite to implement an ML application on an embedded platform that features a GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# To run this notebook, locally as a jupyter notebook, you need to install thhe proper packages.\n",
        "# follow the instructions below to set up your environment.\n",
        "\n",
        "\n",
        "# 1. Crea un entorno virtual usando conda o venv\n",
        "#    Por ejemplo, usando conda:\n",
        "#        conda create -n [myenv] python=3.8\n",
        "#        conda activate [myenv]\n",
        "#    O usando venv:\n",
        "#        python3 -m venv [myenv]\n",
        "#        source [myenv]/bin/activate\n",
        "#\n",
        "# 2. Activa el entorno virtual\n",
        "# 3. Instala los paquetes requeridos usando pip\n",
        "# 4. Ejecuta el notebook\n",
        "\n",
        "\n",
        "# Instala los paquetes requeridos:\n",
        "%pip install numpy -q\n",
        "%pip install pandas -q\n",
        "%pip install matplotlib -q\n",
        "%pip install tensorflow -q\n",
        "%pip install scikit-learn -q\n",
        "%pip install tensorflow-hub -q\n",
        "%pip install tensorflow-datasets -q\n",
        "%pip install tensorflow-estimator -q\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQK0RRRuY3rJ"
      },
      "source": [
        "### Learning outcomes\n",
        "\n",
        "\n",
        "* Explain the differences between a Tensor Flow and a TF Lite environment\n",
        "* Use the TF Lite API to implement an ML application on an embedded GPU\n",
        "* Understand the performance differences between TF and TF Lite\n",
        "* Understand the performance differences between embedded and higher-end GPUs\n",
        "* Use tools for execution time and memory measurement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8wat6Kxul5R"
      },
      "source": [
        "### Basic performance measurement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3LWGJXAzVF1"
      },
      "source": [
        "In this lab you'll be required to measure the execution **time** and **memory** consumption of certains code sequences. There many ways of achieving this and you are encouraged to investigate yourself a bit what options are available and how they differ from each other.\n",
        "\n",
        "When measuring **execution time**, you should think and decide what is the piece of code you are really interested in measuring, and your conclusions must take that into account. Also notice that the shorter time the code takes, the less accurate can be the measurement because any other thing happening in the system will be a significant noise. Besides using more specialized libraries for accurate time measuring, one common trick you can use is to run a short piece of code many times in a loop, measuring the total loop time and diving by the number of iterations. For this lab you could start with the **time.time()** method of the time Python library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xg5VXTVd1-VW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Execution time: 2.2411346435546875e-05 seconds\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# here goes the code you want to measure\n",
        "\n",
        "end = time.time()\n",
        "print(\"Execution time:\", end - start, \"seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmBR84sR2Mqv"
      },
      "source": [
        "With respect to memory consumption, different parts of a program require different amounts of memory. In this lab we are instrested in the maximum amount of memory, or **peak memory**, because it determines an absolute resource requirement that any device that aims to run the code must meet.\n",
        "\n",
        "For this lab you could start with the **memory_info()** method of psutil Python library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KK8NEEGV3FWS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Memory increased by 0.00 MB\n"
          ]
        }
      ],
      "source": [
        "import psutil, os\n",
        "\n",
        "process = psutil.Process(os.getpid())\n",
        "before = process.memory_info().rss\n",
        "\n",
        "# here goes the code you want to measure\n",
        "\n",
        "after = process.memory_info().rss\n",
        "print(f\"Memory increased by {(after - before) / 1024 / 1024:.2f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PIpLax5zH7U"
      },
      "source": [
        "### TensorFlow Lite workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7yMmdHVGlBQ"
      },
      "source": [
        "We have already seen the TF Lite basic workflow in Lab 2.2. We will now use TF Lite on an embedded platform that includes a GPU device to understand its capabilities and limitations.\n",
        "\n",
        "Run the following code on the **Jetson Nano board** and on a **workstation GPU** and compare their **latencies** amd **peak memory** usage. The code **trains** and exports a TF Lite model for classifying the Fashion MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "T52CGZIKD9fL"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-24 17:13:32.126001: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-06-24 17:13:32.251227: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-06-24 17:13:32.335548: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750803212.407067   79189 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750803212.428744   79189 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1750803212.590487   79189 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1750803212.590515   79189 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1750803212.590517   79189 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1750803212.590519   79189 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-06-24 17:13:32.616214: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/home/julian-sanchez/Universidad/Embedded-Machine-Learning/Lab3/myenv/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n",
            "2025-06-24 17:13:37.732592: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-24 17:13:38.336018: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 188160000 exceeds 10% of free system memory.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7833 - loss: 0.6238\n",
            "Epoch 2/2\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8619 - loss: 0.3821\n",
            "\n",
            "Finished model training\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Fashion MNIST dataset\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# Normalize the images to the range [0, 1]\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "# Class names for the Fashion MNIST dataset\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "           \t'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "model = models.Sequential([\n",
        "\tlayers.Flatten(input_shape=(28, 28)),\n",
        "\tlayers.Dense(128, activation='relu'),\n",
        "\tlayers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "          \tloss='sparse_categorical_crossentropy',\n",
        "          \tmetrics=['accuracy'])\n",
        "\n",
        "model.fit(train_images, train_labels, epochs=2)\n",
        "\n",
        "print('\\nFinished model training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AEP7_dDy6ZH"
      },
      "outputs": [],
      "source": [
        "# test model accuracy\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
        "print('\\nTest accuracy:', test_acc)\n",
        "\n",
        "# export model to a keras file\n",
        "model.save(\"fmnist.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6WvSyozdM2r"
      },
      "outputs": [],
      "source": [
        "# load model from a keras file\n",
        "from keras.models import load_model\n",
        "\n",
        "model = load_model(\"fmnist.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMZicM-FUgBf"
      },
      "outputs": [],
      "source": [
        "# convert model to TF Lite\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# save converted model to a file\n",
        "import pathlib\n",
        "tflite_model_file = pathlib.Path('fmnist.tflite')\n",
        "tflite_model_file.write_bytes(tflite_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7a03KRIEED7"
      },
      "source": [
        "Verify that the model files were exported and use them to run **inference**, both on the Jetson Nano and on a **workstation GPU** using **Tensor Flow**. Contrast **latency** and **peak memory** results too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHA8KLonFEfn"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(test_images)\n",
        "\n",
        "# Function to plot the image, its true label, and the predicted label\n",
        "def plot_image(i, predictions_array, true_label, img):\n",
        "\ttrue_label, img = true_label[i], img[i]\n",
        "\tplt.grid(False)\n",
        "\tplt.xticks([])\n",
        "\tplt.yticks([])\n",
        "\n",
        "\tplt.imshow(img, cmap=plt.cm.binary)\n",
        "\n",
        "\tpredicted_label = np.argmax(predictions_array)\n",
        "\tif predicted_label == true_label:\n",
        "\t\tcolor = 'blue'\n",
        "\telse:\n",
        "\t\tcolor = 'red'\n",
        "\n",
        "\tplt.xlabel(f\"{class_names[predicted_label]} ({class_names[true_label]})\", color=color)\n",
        "\n",
        "def plot_value_array(i, predictions_array, true_label):\n",
        "\ttrue_label = true_label[i]\n",
        "\tplt.grid(False)\n",
        "\tplt.xticks(range(10))\n",
        "\tplt.yticks([])\n",
        "\tthisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n",
        "\tplt.ylim([0, 1])\n",
        "\tpredicted_label = np.argmax(predictions_array)\n",
        "\n",
        "\tthisplot[predicted_label].set_color('red')\n",
        "\tthisplot[true_label].set_color('blue')\n",
        "\n",
        "# Plot the first X test images, their predicted labels, and the true labels\n",
        "# Color correct predictions in blue, incorrect predictions in red\n",
        "num_rows = 5\n",
        "num_cols = 3\n",
        "num_images = num_rows * num_cols\n",
        "plt.figure(figsize=(2 * 2 * num_cols, 2 * num_rows))\n",
        "for i in range(num_images):\n",
        "\tplt.subplot(num_rows, 2 * num_cols, 2 * i + 1)\n",
        "\tplot_image(i, predictions[i], test_labels, test_images)\n",
        "\tplt.subplot(num_rows, 2 * num_cols, 2 * i + 2)\n",
        "\tplot_value_array(i, predictions[i], test_labels)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJLFW--THEnz"
      },
      "source": [
        "Finally, run **inference** with **TF Lite** on both platforms and compare with the previous execution's **latency** and **peak memory** usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2212lAtBHXt_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "\n",
        "# Load the Fashion MNIST dataset\n",
        "(_, _), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# Preprocess the test images (normalize and reshape)\n",
        "test_images = test_images.astype(np.float32) / 255.0\n",
        "\n",
        "# Select an image for inference\n",
        "input_image = np.expand_dims(test_images[0], axis=0)\n",
        "\n",
        "tflite_model_file = \"fmnist\"\n",
        "# Load TFLite model and allocate tensors.\n",
        "interpreter = tf.lite.Interpreter(model_path=tflite_model_file+\".tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output tensors.\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "print(input_details)\n",
        "print(output_details)\n",
        "\n",
        "# Set the input tensor\n",
        "interpreter.set_tensor(input_details[0]['index'], input_image)\n",
        "\n",
        "# Run the inference\n",
        "interpreter.invoke()\n",
        "\n",
        "# Get the output tensor\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "# Print the output\n",
        "print(\"Output:\", output_data)\n",
        "\n",
        "# Optionally, you can get the predicted class\n",
        "predicted_class = np.argmax(output_data)\n",
        "print(\"Predicted class:\", predicted_class)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYmxaV2fuYgv"
      },
      "source": [
        "Consolidate all results in one or more tables or plots and write down your conclusions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7k1FsSjhAYt"
      },
      "source": [
        "### CNNs on Tensor Flow Lite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cg_QXRTyIRRp"
      },
      "source": [
        "Now it's your turn to develop a TF Lite application. This time make sure you train a convolutional model to classify images from a different dataset you choose from Kaggle, Roboflow, Hugging Face or similar, not Mnist, Fashion Mnist nor ImageNet. Now, performance evaluation will focus only on **inference**.\n",
        "\n",
        "Use the Jetson Board and the workstation to run inference both with Tensor Flow and TF Lite. Measure the **peak memory** and **time** and estimate the **energy** consumed based on the nominal power consumption of each system or chip."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ib0RDTL4ugB"
      },
      "source": [
        "Use the following code to disable GPU use in TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqIQ-ptk4oBB"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# List available GPUs\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "\n",
        "# Disable all GPUs\n",
        "tf.config.set_visible_devices([], 'GPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRyAL6124rS4"
      },
      "source": [
        "Again, consolidate all results in one or more tables or plots and write down your conclusions."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
